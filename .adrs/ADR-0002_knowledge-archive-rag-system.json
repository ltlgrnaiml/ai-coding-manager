{
  "schema_type": "adr",
  "id": "ADR-0002",
  "title": "Knowledge Archive & RAG System Architecture",
  "status": "active",
  "date": "2025-12-31",
  "review_date": "2026-06-30",
  "author": "Mycahya Eggleston",
  "scope": "core",
  "origin": "Extracted from engineering-tools ADR-0047",
  "context": {
    "description": "LLMs generate generic content without project awareness. Historical context is scattered across files, not queryable, and not linked.",
    "problem": "Need a unified system that: (1) provides persistent storage for sessions, plans, and artifacts, (2) enables semantic search across all project knowledge, (3) injects relevant context into LLM prompts (RAG), (4) tracks LLM usage costs, (5) maintains relationship graph between artifacts."
  },
  "decision": {
    "summary": "Implement SQLite-based Knowledge Archive with hybrid search (BM25 + vector embeddings), content-aware chunking, and LangChain for LLM orchestration.",
    "architecture": {
      "database": "workspace/knowledge.db (SQLite)",
      "layers": [
        {"layer": 1, "name": "Archive", "purpose": "Persistent storage with file sync"},
        {"layer": 2, "name": "Search", "purpose": "Full-text (FTS5) and semantic (vector) search"},
        {"layer": 3, "name": "RAG", "purpose": "Build context from relevant chunks for LLM prompts"}
      ]
    },
    "embedding_model": {
      "primary_local": {"model": "all-mpnet-base-v2", "dimensions": 768},
      "fallback_local": {"model": "all-MiniLM-L6-v2", "dimensions": 384},
      "config": "KNOWLEDGE_EMBEDDING_MODE=local|api"
    },
    "search_strategy": {
      "method": "Hybrid search with Reciprocal Rank Fusion",
      "components": ["FTS5 full-text search", "Cosine similarity on embeddings", "RRF fusion"]
    },
    "pii_sanitization": {
      "method": "Regex-based with configurable patterns",
      "categories": ["API Keys", "Secrets", "Emails", "IPs"]
    }
  },
  "consequences": {
    "positive": [
      "Unified queryable knowledge base",
      "Semantic search enables finding related content by meaning",
      "RAG context injection improves LLM output quality",
      "Local embedding model enables fully offline operation"
    ],
    "negative": [
      "Additional storage for embeddings",
      "Memory usage for embedding model (~500MB)"
    ]
  },
  "guardrails": [
    {"id": "file-ssot", "rule": "Files are always SSOT. Database is cache/index."},
    {"id": "pii-sanitize", "rule": "ALL content MUST pass through PII sanitizer before LLM APIs"},
    {"id": "soft-delete-only", "rule": "Never hard delete documents. Use archived_at."}
  ],
  "tags": ["knowledge-management", "rag", "semantic-search", "embeddings", "sqlite"]
}
